# ğŸ¥ Welding Detector - SzczegÃ³Å‚owa Dokumentacja

## ğŸ“– Spis treÅ›ci
1. [Co to jest i do czego sÅ‚uÅ¼y?](#co-to-jest)
2. [Jak to dziaÅ‚a - architektura](#jak-to-dziaÅ‚a)
3. [GÅ‚Ã³wne funkcje aplikacji](#gÅ‚Ã³wne-funkcje)
4. [UÅ¼yte technologie i biblioteki](#technologie)
5. [Problemy z ktÃ³rymi siÄ™ mierzyliÅ›my](#problemy)
6. [Struktura projektu](#struktura)
7. [Jak uruchomiÄ‡ aplikacjÄ™](#uruchomienie)

---

## ğŸ¯ Co to jest i do czego sÅ‚uÅ¼y? {#co-to-jest}

**Welding Detector** to inteligentna aplikacja do monitorowania i automatycznej kontroli jakoÅ›ci procesu spawania laserowego. WyobraÅº sobie system, ktÃ³ry:

- ğŸ“¹ **Nagrywa proces spawania** z kamery USB w czasie rzeczywistym
- ğŸ¤– **Automatycznie wykrywa wady** spawÃ³w uÅ¼ywajÄ…c sztucznej inteligencji
- âœ‚ï¸ **Wycina niepotrzebne fragmenty** filmÃ³w (np. sam moment spawania)
- ğŸ“Š **Analizuje caÅ‚e nagrania** i pokazuje statystyki: ile spawÃ³w byÅ‚o OK, ile NOK
- ğŸ·ï¸ **Pozwala oznaczaÄ‡ klatki** i trenowaÄ‡ wÅ‚asne modele AI

Jest to system wizyjny do **kontroli jakoÅ›ci produkcji** - zamiast czÅ‚owieka przeglÄ…dajÄ…cego kaÅ¼dy spaw, AI robi to automatycznie i bÅ‚yskawicznie.

---

## ğŸ—ï¸ Jak to dziaÅ‚a - Architektura {#jak-to-dziaÅ‚a}

Aplikacja skÅ‚ada siÄ™ z **dwÃ³ch gÅ‚Ã³wnych czÄ™Å›ci**:

### 1ï¸âƒ£ Backend (Serwer) - Python
To "silnik" aplikacji, ktÃ³ry robi caÅ‚Ä… ciÄ™Å¼kÄ… pracÄ™:
- ÅÄ…czy siÄ™ z kamerÄ… USB i przechwytuje obraz
- Nagrywa wideo do plikÃ³w MP4
- Uruchamia modele AI do wykrywania wad
- Przetwarza filmy (wykrywa ruch, wycina spawanie)
- Odpowiada na Å¼Ä…dania frontendu przez API

**Technologia:** Python + FastAPI (szybki framework webowy)

### 2ï¸âƒ£ Frontend (Interfejs uÅ¼ytkownika) - Vue.js
To strona internetowa, ktÃ³rÄ… widzisz w przeglÄ…darce:
- Pokazuje obraz z kamery na Å¼ywo (live stream)
- Ma przyciski do nagrywania, analizy, przeglÄ…dania wynikÃ³w
- WyÅ›wietla Å‚adne wykresy i statystyki
- Pozwala oznaczaÄ‡ zdjÄ™cia spawÃ³w (OK/NOK)

**Technologia:** Vue.js 3 + Vite + TailwindCSS (nowoczesny stack frontendowy)

### ğŸ”„ Jak siÄ™ komunikujÄ…?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          HTTP/REST API           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend (Vue)    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚   Backend (Python)   â”‚
â”‚  (przeglÄ…darka)     â”‚                                  â”‚      FastAPI         â”‚
â”‚                     â”‚  GET /camera/stream - pobierz    â”‚                      â”‚
â”‚  - Live stream      â”‚  POST /recording/start - nagraj  â”‚  - Kamera USB        â”‚
â”‚  - Przyciski        â”‚  POST /ml/analyze - analizuj     â”‚  - Nagrywanie MP4    â”‚
â”‚  - Wyniki analizy   â”‚  GET /recording/list - lista     â”‚  - Modele AI         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Frontend wysyÅ‚a **Å¼Ä…dania HTTP** (jak: "daj mi listÄ™ nagraÅ„"), a backend odpowiada **JSON** (strukturÄ… danych).

---

## ğŸš€ GÅ‚Ã³wne funkcje aplikacji {#gÅ‚Ã³wne-funkcje}

### ğŸ“¹ 1. Live Streaming i nagrywanie

**Co robi:**
- Pokazuje obraz z kamery USB w czasie rzeczywistym
- Pozwala nagrywaÄ‡ wideo do pliku MP4
- NakÅ‚ada timestamp (datÄ™ i godzinÄ™) oraz znacznik "REC" na obraz

**Jak to dziaÅ‚a technicznie:**

1. **Background Capture Thread** - osobny wÄ…tek w Pythonie, ktÃ³ry non-stop pobiera klatki z kamery:
```python
while running:
    ret, frame = cap.read()  # Pobierz klatkÄ™ z kamery
    # Konwertuj do JPEG
    _, buf = cv2.imencode('.jpg', frame, quality=95)
    last_frame = buf.tobytes()  # Zapisz do bufora
```

2. **MJPEG Streaming** - kaÅ¼da klatka jest wysyÅ‚ana jako JPEG przez HTTP:
```
--frame
Content-Type: image/jpeg

[DANE JPEG KLATKI]
--frame
Content-Type: image/jpeg

[NASTÄ˜PNA KLATKA]
...
```

3. **Nagrywanie do MP4** - podczas nagrywania klatki sÄ… zapisywane przez OpenCV:
```python
video_writer = cv2.VideoWriter('nagranie.mp4', codec='mp4v', fps=30)
video_writer.write(frame)  # Zapisz kaÅ¼dÄ… klatkÄ™
```

**Problem ktÃ³ry rozwiÄ…zaliÅ›my:**
- Pierwsze wersje miaÅ‚y **opÃ³Åºnienie** (lag) 3-5 sekund
- **RozwiÄ…zanie:** UÅ¼ycie backend MSMF (Media Foundation) + format MJPEG + bufor 1 klatka
- Wynik: OpÃ³Åºnienie spadÅ‚o do ~0.1s

### ğŸ¤– 2. Automatyczna analiza wad (Machine Learning)

**Co robi:**
- Analizuje kaÅ¼dÄ… klatkÄ™ wideo i okreÅ›la: **OK** (dobry spaw) lub **NOK** (wada)
- JeÅ›li NOK, klasyfikuje **typ wady**: pÄ™kniÄ™cie, porowatoÅ›Ä‡, brak przetopu, itp.
- Pokazuje **procentowÄ… pewnoÅ›Ä‡** przewidywania (np. NOK 95%)

**Jak to dziaÅ‚a:**

1. **Model binarny (OK/NOK)** - EfficientNet-B0:
```
ZdjÄ™cie spawu â†’ SieÄ‡ neuronowa â†’ [0.85 OK, 0.15 NOK] â†’ Wynik: OK (85%)
```

2. **Model defektÃ³w (typ wady)** - teÅ¼ EfficientNet-B0:
```
ZdjÄ™cie wady â†’ SieÄ‡ neuronowa â†’ [crack: 0.92, porosity: 0.05, ...] â†’ Wynik: pÄ™kniÄ™cie (92%)
```

**Co to jest EfficientNet-B0?**
To gotowa architektura sieci neuronowej (z biblioteki `timm`), ktÃ³ra jest:
- **Szybka** - moÅ¼e analizowaÄ‡ wiele zdjÄ™Ä‡ na sekundÄ™
- **DokÅ‚adna** - nauczona na milionach zdjÄ™Ä‡ (ImageNet)
- **MaÅ‚a** - waÅ¼y ~20MB, dziaÅ‚a nawet na CPU

**Co to jest Grad-CAM?**
To wizualizacja "na co patrzy AI" - nakÅ‚ada heatmapÄ™ pokazujÄ…cÄ…, ktÃ³re fragmenty zdjÄ™cia wpÅ‚ynÄ™Å‚y na decyzjÄ™:

```
Oryginalne zdjÄ™cie + Heatmapa = WidaÄ‡ gdzie AI znalazÅ‚ wadÄ™ (np. pÄ™kniÄ™cie Å›wieci na czerwono)
```

**Trenowanie modelu:**
- UÅ¼ytkownik oznacza klatki (ğŸ·ï¸ Labeling) jako OK/NOK + typ wady
- Po zebraniu np. 100+ zdjÄ™Ä‡ klikasz "ğŸ‹ï¸ Trenuj model"
- Aplikacja uruchamia trening (10-30 minut na CPU, 2-5 min na GPU)
- Nowy model jest zapisywany i od razu uÅ¼ywany do przewidywaÅ„

### ğŸ“Š 3. Batch analiza nagraÅ„

**Co robi:**
- Analizuje **caÅ‚e wideo** klatka po klatce
- Tworzy raport: ile OK, ile NOK, jakie typy wad
- Pokazuje miniatury wszystkich klatek NOK
- Wyniki sÄ… zapisywane i widoczne nawet po odÅ›wieÅ¼eniu strony

**Jak to dziaÅ‚a:**

1. Klikasz "ğŸ”¬ Analizuj wideo"
2. Backend:
```python
for frame in video:
    prediction = ml_model.predict(frame)  # OK czy NOK?
    if prediction == "nok":
        defect_type = defect_model.predict(frame)  # Jaki typ wady?
    results.append(...)
```
3. Wyniki sÄ… zapisywane do `recordings/analysis/{filename}.json`
4. Frontend odczytuje i wyÅ›wietla statystyki + miniatury

**Optymalizacja:**
- Parametr `skip_frames=5` - analizuj co 5. klatkÄ™ (30x szybciej, prawie taka sama dokÅ‚adnoÅ›Ä‡)

**Przechowywanie wynikÃ³w:**
Aby wyniki nie znikaÅ‚y po odÅ›wieÅ¼eniu strony, uÅ¼ywamy **localStorage** w przeglÄ…darce:
```javascript
localStorage.setItem('analysisResults', JSON.stringify(wyniki))
// Po odÅ›wieÅ¼eniu:
wyniki = JSON.parse(localStorage.getItem('analysisResults'))
```

### âœ‚ï¸ 4. Trim to Motion - Wycinanie zbÄ™dnych fragmentÃ³w

**Co robi:**
- Automatycznie wykrywa momenty **ruchu** w nagraniu
- Wycina statyczne fragmenty (kiedy nic siÄ™ nie dzieje)
- Zapisuje krÃ³tsze wideo z samÄ… akcjÄ…

**Jak wykrywa ruch:**

1. PorÃ³wnuje sÄ…siednie klatki:
```python
previous_frame = klatka[0]
current_frame = klatka[1]
difference = abs(current_frame - previous_frame)  # RÃ³Å¼nica pikseli
if difference > threshold:
    # Ruch wykryty!
```

2. Grupuje klatki z ruchem w **segmenty**:
```
Klatki: [0..10 statyczne] [11..50 RUCH] [51..60 statyczne] [61..100 RUCH]
Segmenty: [(11, 50), (61, 100)]
```

3. Zapisuje tylko segmenty z ruchem do nowego pliku MP4

**Padding:** Dodaje 30 klatek przed ruchem i 5 po (Å¼eby nie uciÄ…Ä‡ za wczeÅ›nie/pÃ³Åºno)

### ğŸ”¥ 5. Trim to Post-Processing - Usuwanie spawania

**Co robi:**
- Wykrywa moment **aktywnego spawania** (jasny laser)
- **Wycina TYLKO spawanie**, zostawia przygotowanie i inspekcjÄ™
- Idealny do przeglÄ…dania gotowych spawÃ³w bez oÅ›lepiajÄ…cego lasera

**Jak wykrywa spawanie:**

Analizuje kaÅ¼dÄ… klatkÄ™ pod kÄ…tem jasnoÅ›ci i koloru:

```python
# Metoda 1: Bardzo jasne piksele (biaÅ‚e/Å¼Ã³Å‚te centrum lasera)
very_bright_pixels = count(pixels > 220)

# Metoda 2: Czerwone/pomaraÅ„czowe Å›wiatÅ‚o (rozÅ¼arzony metal)
red_hot_pixels = count(R>220 AND G>180 AND B<120)

# JeÅ›li ktÃ³rykolwiek warunek speÅ‚niony = spawanie
if very_bright_pixels >= 1% OR red_hot_pixels >= 3%:
    welding_detected = True
```

**Grupowanie z tolerancjÄ…:**
```
Gap tolerance = 10 klatek (0.3s)

Jasne klatki: [10, 11, 12, 25, 26, 27] â† przerwa 13 klatek
Segmenty: [(10, 12), (25, 27)]  â† dwa osobne segmenty

Jasne klatki: [10, 11, 12, 18, 19, 20] â† przerwa 6 klatek
Segmenty: [(10, 20)]  â† jeden ciÄ…gÅ‚y segment
```

**Przypadki brzegowe:**
- Nie wykryto spawania â†’ zachowaj caÅ‚e wideo
- >80% wideo to spawanie â†’ zachowaj drugÄ… poÅ‚owÄ™ (inspekcja)
- Spawanie na koÅ„cu â†’ zachowaj poczÄ…tek (przygotowanie)

**Problem ktÃ³ry rozwiÄ…zaliÅ›my:**
- PoczÄ…tkowo algorytm wykrywaÅ‚ **czerwonÄ… poÅ›wiatÄ™ po zgaÅ›niÄ™ciu lasera** jako spawanie
- Zbyt duÅ¼o materiaÅ‚u byÅ‚o wycinane zaraz po spawaniu
- **RozwiÄ…zania:**
  - Zmniejszono gap_tolerance z 30 do 10 klatek (wykrywanie koÅ„czy siÄ™ szybciej)
  - UsuniÄ™to buffer na koÅ„cu spawania (dokÅ‚adny moment zgaÅ›niÄ™cia)
  - ZwiÄ™kszono progi jasnoÅ›ci (tylko bardzo jasne Å›wiatÅ‚o = aktywny laser)

### ğŸ·ï¸ 6. Labeling - Oznaczanie danych treningowych

**Co robi:**
- Pozwala rÄ™cznie oznaczaÄ‡ klatki jako OK/NOK
- Dla NOK moÅ¼na wybraÄ‡ typ wady z 9 kategorii
- Zbiera dane do treningu modeli AI

**Typy wad:**
1. ğŸ«§ PorowatoÅ›Ä‡ (porosity) - pÄ™cherzyki powietrza
2. ğŸ’” PÄ™kniÄ™cie (crack) - rysy, szczeliny
3. ğŸ”— Brak przetopu (lack_of_fusion) - materiaÅ‚ siÄ™ nie poÅ‚Ä…czyÅ‚
4. ğŸ“‰ Podtopienie (undercut) - wgÅ‚Ä™bienie
5. ğŸ”¥ Przepalenie (burn_through) - dziura
6. ğŸ’¦ Rozpryski (spatter) - rozbryzgi metalu
7. ã€°ï¸ NierÃ³wna spoina (irregular_bead)
8. ğŸ¦  Zanieczyszczenie (contamination)
9. â“ Inna wada (other)

**Workflow:**
```
1. OtwÃ³rz Frame Viewer
2. Wybierz klatkÄ™
3. Kliknij "OK" lub "NOK"
4. JeÅ›li NOK â†’ wybierz typ wady
5. Auto-przejÅ›cie do nastÄ™pnej klatki
6. Po 100+ oznaczonych â†’ trenuj model
```

### âš™ï¸ 7. Ustawienia kamery

**Co moÅ¼na zmieniÄ‡:**
- **RozdzielczoÅ›Ä‡:** 1280x720 (HD) lub 1920x1080 (FHD)
- **FPS:** 30 lub 60 klatek na sekundÄ™
- **JakoÅ›Ä‡ JPEG:** 50-100% (wpÅ‚ywa na rozmiar pliku)
- **Kontrast:** 0-255 (dla ciemnych/jasnych scen)
- **Monochromatyczny:** czarno-biaÅ‚y obraz

**Rzeczywisty vs. Å»Ä…dany FPS:**

Aplikacja **mierzy rzeczywisty FPS** kamery:
```python
start = time.now()
for i in range(60):
    cap.read()  # Pobierz 60 klatek
elapsed = time.now() - start
actual_fps = 60 / elapsed  # Np. 60 / 2.1s = 28.5 FPS
```

Dlaczego? Kamera moÅ¼e nie wspieraÄ‡ Å¼Ä…danych 60 FPS - wtedy dostajemy np. 30 FPS. Musimy to wiedzieÄ‡, Å¼eby poprawnie zapisaÄ‡ wideo (inaczej odtwarzane jest za szybko/wolno).

---

## ğŸ› ï¸ Technologie i biblioteki {#technologie}

### Backend (Python)

#### 1. **FastAPI** - Framework webowy
```python
from fastapi import FastAPI
app = FastAPI()

@app.get("/camera/stream")
async def stream():
    return StreamingResponse(...)
```
**Po co:** Tworzenie REST API - endpointÃ³w, ktÃ³re odpowiadajÄ… na Å¼Ä…dania HTTP.
- Szybki (async/await)
- Automatyczna dokumentacja (Swagger UI)
- Walidacja typÃ³w (Pydantic)

#### 2. **OpenCV (cv2)** - Przetwarzanie obrazu i wideo
```python
import cv2
cap = cv2.VideoCapture(0)  # OtwÃ³rz kamerÄ™
ret, frame = cap.read()     # Pobierz klatkÄ™
cv2.imwrite('frame.jpg', frame)  # Zapisz jako JPEG
```
**Po co:** Wszystko zwiÄ…zane z kamerÄ… i wideo:
- Przechwytywanie z USB
- Kodowanie/dekodowanie JPEG, MP4
- Przetwarzanie obrazu (konwersja kolorÃ³w, blur, threshold)
- Wykrywanie ruchu (absdiff)

#### 3. **PyTorch** - Deep Learning
```python
import torch
model = torch.load('model.pth')
prediction = model(image)  # [0.85 OK, 0.15 NOK]
```
**Po co:** Uruchamianie sieci neuronowych do klasyfikacji.
- GPU acceleration (CUDA)
- Automatyczne rÃ³Å¼niczkowanie (autograd)
- Transfer learning (wykorzystanie gotowych modeli)

#### 4. **timm** (PyTorch Image Models)
```python
import timm
model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=2)
```
**Po co:** Biblioteka z gotowymi architekturami sieci (EfficientNet, ResNet, Vision Transformer).
- Pretrained weights (wytrenowane na ImageNet)
- Setki modeli "out of the box"

#### 5. **torchvision** - Transformacje obrazÃ³w
```python
from torchvision import transforms
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406])
])
```
**Po co:** Przygotowanie obrazÃ³w do sieci neuronowej (resize, normalizacja).

#### 6. **NumPy** - Obliczenia numeryczne
```python
import numpy as np
arr = np.array([1, 2, 3])
mean = np.mean(arr)  # 2.0
```
**Po co:** Szybkie operacje na tablicach (klatki wideo to tablice NumPy).

#### 7. **Uvicorn** - Serwer ASGI
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```
**Po co:** Uruchamia aplikacjÄ™ FastAPI.

---

### Frontend (Vue.js)

#### 1. **Vue.js 3** - Framework JavaScript
```vue
<template>
  <button @click="startRecording">ğŸ”´ Nagrywaj</button>
</template>

<script setup>
function startRecording() {
  fetch('/recording/start', { method: 'POST' })
}
</script>
```
**Po co:** Tworzenie reaktywnego UI.
- Reactive state (ref, reactive)
- Komponenty (modularne UI)
- Two-way binding (v-model)

#### 2. **Vite** - Build tool
```bash
npm run dev  # Szybki dev server z HMR
npm run build  # Produkcyjny build
```
**Po co:** Szybkie budowanie i hot reload podczas developmentu.

#### 3. **TailwindCSS** - Style CSS
```html
<button class="bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded">
  Kliknij mnie
</button>
```
**Po co:** Utility-first CSS - szybkie stylowanie bez pisania CSS.

#### 4. **Fetch API** - HTTP requesty
```javascript
const response = await fetch('/api/endpoint')
const data = await response.json()
```
**Po co:** Komunikacja z backendem (pobieranie/wysyÅ‚anie danych).

---

## ğŸ› Problemy z ktÃ³rymi siÄ™ mierzyliÅ›my {#problemy}

### Problem 1: OpÃ³Åºnienie stream kamery (3-5 sekund lag)

**Objawy:**
- Ruszasz rÄ™kÄ… przed kamerÄ…, a na ekranie pojawia siÄ™ to 3 sekundy pÃ³Åºniej
- NiemoÅ¼liwoÅ›Ä‡ precyzyjnego pozycjonowania

**Przyczyna:**
- DomyÅ›lny backend OpenCV (Auto) buforowaÅ‚ wiele klatek
- Kamera wysyÅ‚aÅ‚a raw BGR, co byÅ‚o wolne przez USB

**RozwiÄ…zanie:**
1. **Zmiana backend na MSMF** (Media Foundation):
```python
cap = cv2.VideoCapture(0, cv2.CAP_MSMF)  # Najszybszy na Windows
```

2. **Format MJPEG** (sprzÄ™towa kompresja):
```python
cap.set(cv2.CAP_PROP_FOURCC, 'MJPG')  # Kamera kompresuje JPEG, nie CPU
```

3. **Minimalny bufor**:
```python
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Tylko 1 klatka w buforze
```

**Wynik:** Lag spadÅ‚ z 3-5s do ~0.1s âœ…

---

### Problem 2: NieprawidÅ‚owa prÄ™dkoÅ›Ä‡ odtwarzania nagraÅ„

**Objawy:**
- Nagranie odtwarza siÄ™ za szybko (jak w przyspieszeniu)
- Å»Ä…dane 30 FPS, a film leci jak 60 FPS

**Przyczyna:**
- Kamera deklaruje 60 FPS, ale faktycznie daje 30 FPS
- VideoWriter zapisuje z Å¼Ä…danym FPS, nie rzeczywistym

**RozwiÄ…zanie:**
Pomiar rzeczywistego FPS:
```python
def _measure_actual_fps():
    start = time.perf_counter()
    frames = 0
    for _ in range(60):
        if cap.read()[0]:
            frames += 1
    elapsed = time.perf_counter() - start
    actual_fps = frames / elapsed
```

UÅ¼ycie zmierzonego FPS przy nagrywaniu:
```python
writer = cv2.VideoWriter('video.mp4', codec, self.actual_fps, (width, height))
```

**Wynik:** Nagrania odtwarzajÄ… siÄ™ z prawidÅ‚owÄ… prÄ™dkoÅ›ciÄ… âœ…

---

### Problem 3: Procentowe pewnoÅ›ci pokazywaÅ‚y 1000%

**Objawy:**
- Model zwraca 85% pewnoÅ›ci
- Na ekranie: "850%" lub "1000%"

**Przyczyna:**
- Model PyTorch zwraca `confidence` w zakresie 0-100
- Frontend dodatkowo mnoÅ¼yÅ‚ * 100

**RozwiÄ…zanie:**
```javascript
// âŒ ByÅ‚o:
{{ prediction.confidence * 100 }}%

// âœ… Jest:
{{ prediction.confidence }}%
```

**Wynik:** Poprawne wyÅ›wietlanie procentÃ³w âœ…

---

### Problem 4: Wyniki analizy znikaÅ‚y po odÅ›wieÅ¼eniu strony

**Objawy:**
- Przeanalizujesz wideo, wszystko dziaÅ‚a
- OdÅ›wieÅ¼ysz stronÄ™ F5 â†’ wyniki zniknÄ™Å‚y

**Przyczyna:**
- Wyniki byÅ‚y trzymane tylko w zmiennej `recordings.value` (pamiÄ™Ä‡ RAM)
- Po odÅ›wieÅ¼eniu strony wszystko siÄ™ resetowaÅ‚o

**RozwiÄ…zanie:**
Zapisywanie do **localStorage** przeglÄ…darki:
```javascript
// Po zakoÅ„czeniu analizy:
function saveAnalysisResults() {
  const data = {}
  recordings.value.forEach(rec => {
    if (rec.analysis?.results) {
      data[rec.filename] = rec.analysis
    }
  })
  localStorage.setItem('analysisResults', JSON.stringify(data))
}

// Po zaÅ‚adowaniu strony:
function restoreAnalysisResults() {
  const saved = localStorage.getItem('analysisResults')
  if (saved) {
    const data = JSON.parse(saved)
    recordings.value.forEach(rec => {
      if (data[rec.filename]) {
        rec.analysis = data[rec.filename]
      }
    })
  }
}
```

**Wynik:** Wyniki przetrwajÄ… odÅ›wieÅ¼enie i zamkniÄ™cie przeglÄ…darki âœ…

---

### Problem 5: Wykrywanie spawania wycinaÅ‚o za duÅ¼o (czerwona poÅ›wiata)

**Objawy:**
- Po zgaÅ›niÄ™ciu lasera metal jeszcze czerwono Å›wieci
- Algorytm wykrywaÅ‚ to jako spawanie i wycinaÅ‚
- Za maÅ‚o materiaÅ‚u po spawaniu do inspekcji

**Przyczyna:**
- Gap tolerance = 30 klatek (1s) â†’ algorytm kontynuowaÅ‚ wykrywanie przez 1s po ostatniej jasnej klatce
- Buffer +3 klatki na koÅ„cu spawania
- Niskie progi detekcji (kaÅ¼da czerwona poÅ›wiata = spawanie)

**RozwiÄ…zanie (iteracyjny proces):**

**Iteracja 1:** Zmniejsz gap_tolerance
```python
gap_tolerance = 30  # ByÅ‚o
gap_tolerance = 10  # Jest (0.3s zamiast 1s)
```

**Iteracja 2:** UsuÅ„ buffer na koÅ„cu
```python
# ByÅ‚o:
weld_end_buffered = weld_end + buffer_frames

# Jest:
weld_end_buffered = weld_end  # DokÅ‚adny moment zgaÅ›niÄ™cia
```

**Iteracja 3:** Bardziej restrykcyjne progi
```python
# Wykrywaj tylko bardzo jasne Å›wiatÅ‚o (aktywny laser)
very_bright_pixels > 220 AND >= 1%  # BiaÅ‚e/Å¼Ã³Å‚te centrum
red_hot >= 3%  # Intensywna czerwieÅ„
```

**Wynik:** DokÅ‚adne wykrywanie koÅ„ca spawania âœ…

---

### Problem 6: Trim to motion zostawiaÅ‚ za duÅ¼o klatek na koÅ„cu

**Objawy:**
- Po zakoÅ„czeniu ruchu nagranie trwa jeszcze 0.5s
- Statyczne klatki na koÅ„cu

**Przyczyna:**
- Padding na koÅ„cu segmentu = 30 klatek (0.5s przy 60 FPS)

**RozwiÄ…zanie:**
```python
# ByÅ‚o:
seg_end = end + self.padding_frames  # +30 klatek

# Jest:
seg_end = end + 5  # Tylko 5 klatek (~0.08s)
```

Zachowujemy peÅ‚ny padding (30 klatek) na **poczÄ…tku** ruchu (Å¼eby zÅ‚apaÄ‡ start), ale minimalny (5 klatek) na **koÅ„cu**.

**Wynik:** Precyzyjne uciÄ™cie na koÅ„cu ruchu âœ…

---

## ğŸ“ Struktura projektu {#struktura}

```
welding-detector/
â”œâ”€â”€ app/                          # Backend (Python/FastAPI)
â”‚   â”œâ”€â”€ main.py                   # Entry point - uruchamia serwer
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py           # Konfiguracja (porty, FPS, rozdzielczoÅ›Ä‡)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py             # Endpointy API (40+ endpointÃ³w)
â”‚   â”‚   â””â”€â”€ models.py             # Modele Pydantic (Request/Response)
â”‚   â””â”€â”€ services/                 # Logika biznesowa
â”‚       â”œâ”€â”€ camera_service.py               # Kamera USB + streaming + nagrywanie
â”‚       â”œâ”€â”€ frame_overlay_service.py        # Timestamp + REC overlay
â”‚       â”œâ”€â”€ video_overlay_service.py        # Overlay dla gotowych filmÃ³w
â”‚       â”œâ”€â”€ motion_detection_service.py     # Wykrywanie ruchu + spawania
â”‚       â”œâ”€â”€ frame_extractor_service.py      # Ekstrakcja klatek z MP4
â”‚       â”œâ”€â”€ image_enhancement_service.py    # Filtry (CLAHE, sharpen, denoise)
â”‚       â”œâ”€â”€ labeling_service.py             # Oznaczanie OK/NOK + wady
â”‚       â”œâ”€â”€ ml_classification_service.py    # Model OK/NOK + trening
â”‚       â”œâ”€â”€ defect_classifier_service.py    # Model typÃ³w wad + trening
â”‚       â””â”€â”€ video_analysis_service.py       # Batch analiza caÅ‚ych filmÃ³w
â”‚
â”œâ”€â”€ app_frontend/                 # Frontend (Vue.js 3)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.vue               # GÅ‚Ã³wny komponent (1800+ linii)
â”‚   â”‚   â”œâ”€â”€ main.js               # Entry point Vue
â”‚   â”‚   â””â”€â”€ style.css             # Style globalne + Tailwind
â”‚   â”œâ”€â”€ index.html                # Szablon HTML
â”‚   â”œâ”€â”€ package.json              # ZaleÅ¼noÅ›ci npm
â”‚   â””â”€â”€ vite.config.js            # Konfiguracja Vite
â”‚
â”œâ”€â”€ recordings/                   # Nagrania wideo
â”‚   â”œâ”€â”€ *.mp4                     # Pliki wideo
â”‚   â””â”€â”€ analysis/                 # Wyniki analiz
â”‚       â””â”€â”€ *.json                # {filename: {summary, frames, defects}}
â”‚
â”œâ”€â”€ labels/                       # Dane treningowe
â”‚   â””â”€â”€ training_data/
â”‚       â”œâ”€â”€ ok/                   # ZdjÄ™cia OK
â”‚       â”œâ”€â”€ nok/                  # ZdjÄ™cia NOK
â”‚       â””â”€â”€ defects/              # Typy wad
â”‚           â”œâ”€â”€ porosity/
â”‚           â”œâ”€â”€ crack/
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                       # Wytrenowane modele AI
â”‚   â”œâ”€â”€ latest_model.pth          # Model binarny OK/NOK
â”‚   â”œâ”€â”€ training_info.json        # Metryki (accuracy, loss)
â”‚   â””â”€â”€ defects/
â”‚       â”œâ”€â”€ defect_classifier.pth # Model 9-klasowy (typy wad)
â”‚       â””â”€â”€ training_info.json
â”‚
â”œâ”€â”€ requirements.txt              # ZaleÅ¼noÅ›ci Python
â”œâ”€â”€ pytest.ini                    # Konfiguracja testÃ³w
â”œâ”€â”€ Dockerfile                    # Backend Docker
â”œâ”€â”€ docker-compose.yml            # Orchestracja (backend + frontend)
â”œâ”€â”€ README.md                     # Oryginalny README
â””â”€â”€ README2.md                    # Ten dokument ğŸ‘ˆ
```

---

## ğŸš€ Jak uruchomiÄ‡ aplikacjÄ™ {#uruchomienie}

### Wymagania

**SprzÄ™t:**
- Kamera USB (kompatybilna z Windows DirectShow/MSMF)
- CPU: Intel i5/Ryzen 5 lub lepszy
- RAM: 8GB minimum, 16GB zalecane
- (Opcjonalnie) GPU NVIDIA z CUDA dla szybszego treningu

**Oprogramowanie:**
- Windows 10/11
- Python 3.9+ (zalecane 3.11)
- Node.js 18+ (dla frontendu)
- Git

---

### Metoda 1: RÄ™czne uruchomienie

#### Backend (Terminal 1)

```bash
# 1. Sklonuj repozytorium
git clone <repo-url>
cd welding-detector

# 2. UtwÃ³rz wirtualne Å›rodowisko Python
python -m venv venv
venv\Scripts\activate  # Windows

# 3. Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt

# 4. Uruchom serwer
python -m app.main
# Lub:
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# âœ… Backend dziaÅ‚a na http://localhost:8000
# Dokumentacja API: http://localhost:8000/docs
```

#### Frontend (Terminal 2)

```bash
# 1. PrzejdÅº do folderu frontendu
cd app_frontend

# 2. Zainstaluj zaleÅ¼noÅ›ci npm
npm install

# 3. Uruchom dev server
npm run dev

# âœ… Frontend dziaÅ‚a na http://localhost:3000
```

#### OtwÃ³rz przeglÄ…darkÄ™

```
http://localhost:3000
```

PowinieneÅ› zobaczyÄ‡:
- Live stream z kamery
- Przyciski nagrywania, analizy, etc.

---

### Metoda 2: Docker (jeÅ›li skonfigurowane)

```bash
docker-compose up --build
```

---

## ğŸ“ NajwaÅ¼niejsze pojÄ™cia wyjaÅ›nione prostymi sÅ‚owami

### ğŸ¤– SieÄ‡ neuronowa (Neural Network)
WyobraÅº sobie wiele warstw filtrÃ³w, ktÃ³re uczÄ… siÄ™ rozpoznawaÄ‡ wzorce na zdjÄ™ciach:
- Warstwa 1: wykrywa krawÄ™dzie
- Warstwa 2: wykrywa ksztaÅ‚ty (okrÄ…g, linia)
- Warstwa 3: wykrywa tekstury (metal, pÄ™kniÄ™cie)
- Warstwa N: "To jest pÄ™kniÄ™cie spawu!"

### ğŸ“Š Trening modelu (Training)
Pokazujesz komputerowi 1000 zdjÄ™Ä‡: "To jest OK, to jest NOK, to jest pÄ™kniÄ™cie..."
Komputer dostosowuje parametry (miliony liczb), Å¼eby siÄ™ nauczyÄ‡ rozpoznawaÄ‡ wzorce.

### ğŸ”® Przewidywanie (Inference)
Pokazujesz wytrenowanemu modelowi nowe zdjÄ™cie â†’ on mÃ³wi: "NOK z pewnoÅ›ciÄ… 92%".

### ğŸ—ºï¸ Grad-CAM (Gradient-weighted Class Activation Mapping)
Wizualizacja "na co patrzy AI":
- Czerwone obszary = tu model widzi wadÄ™
- Niebieskie obszary = niewaÅ¼ne dla decyzji

### ğŸï¸ FPS (Frames Per Second)
Ile zdjÄ™Ä‡ (klatek) na sekundÄ™:
- 30 FPS = 30 zdjÄ™Ä‡/sekundÄ™ = pÅ‚ynny obraz
- 60 FPS = 60 zdjÄ™Ä‡/sekundÄ™ = bardzo pÅ‚ynny
- Oko ludzkie widzi ~24 FPS jako pÅ‚ynny ruch

### ğŸ“¦ MJPEG (Motion JPEG)
Format wideo gdzie kaÅ¼da klatka to osobny JPEG:
- Klatka 1: JPEG
- Klatka 2: JPEG
- Klatka 3: JPEG
- ...

Zalety: Szybkie, sprzÄ™towe wsparcie w kamerach USB

### ğŸ¥ MP4 (H.264/H.265)
Format wideo ze skompresjÄ…:
- Zapisuje peÅ‚nÄ… klatkÄ™ co kilka sekund (I-frame)
- Reszta to rÃ³Å¼nice (P-frames, B-frames)
- Mniejsze pliki niÅ¼ MJPEG, ale wolniejsze przetwarzanie

### ğŸŒŠ Streaming
CiÄ…gÅ‚e wysyÅ‚anie danych (wideo/audio) kawaÅ‚ek po kawaÅ‚ku:
```
Kamera â†’ [klatka 1] â†’ PrzeglÄ…darka (wyÅ›wietl)
       â†’ [klatka 2] â†’ PrzeglÄ…darka (wyÅ›wietl)
       â†’ [klatka 3] â†’ PrzeglÄ…darka (wyÅ›wietl)
       ...
```

### ğŸ”„ Async/Await (AsynchronicznoÅ›Ä‡)
WielozadaniowoÅ›Ä‡ bez blokowania:
```python
async def pobierz_dane():
    await fetch(url)  # Czekaj, ale nie blokuj innych zadaÅ„
```

Jak kucharz gotujÄ…cy kilka potraw naraz (nie czeka aÅ¼ woda siÄ™ zagotuje, lecz robi coÅ› innego).

### ğŸ¯ REST API
SposÃ³b komunikacji frontend â†” backend przez HTTP:
```
GET /camera/stream â†’ pobierz stream
POST /recording/start â†’ rozpocznij nagrywanie
GET /recording/list â†’ pobierz listÄ™ nagraÅ„
DELETE /recording/xyz.mp4 â†’ usuÅ„ nagranie
```

### ğŸ“¡ HTTP Request/Response
```
[Frontend]                    [Backend]
    |                             |
    |  GET /camera/health  --->  |
    |                             | (sprawdÅº kamerÄ™)
    |  <--- 200 OK                |
    |  { "status": "healthy" }    |
```

---

## ğŸ‰ Podsumowanie

**Welding Detector** to zaawansowany system kontroli jakoÅ›ci spawania, ktÃ³ry Å‚Ä…czy:

âœ… **Monitoring w czasie rzeczywistym** (live stream z kamery)  
âœ… **Nagrywanie wideo** z overlay timestamp  
âœ… **SztucznÄ… inteligencjÄ™** (EfficientNet + PyTorch)  
âœ… **AutomatycznÄ… analizÄ™** caÅ‚ych nagraÅ„  
âœ… **Inteligentne przycinanie** (ruch, spawanie)  
âœ… **Trenowanie wÅ‚asnych modeli** (transfer learning)  
âœ… **Intuicyjny interfejs** (Vue.js + TailwindCSS)  

Projekt rozwija siÄ™ iteracyjnie, rozwiÄ…zujÄ…c realne problemy:
- OpÃ³Åºnienie streamu
- Precyzyjne wykrywanie spawania
- Persystencja wynikÃ³w
- Optymalizacja wydajnoÅ›ci

**Technologie:**
- **Backend:** Python, FastAPI, OpenCV, PyTorch, NumPy
- **Frontend:** Vue.js 3, Vite, TailwindCSS
- **AI:** EfficientNet-B0, Grad-CAM, Transfer Learning
- **Video:** MJPEG streaming, MP4 encoding/decoding, motion detection

---

## ğŸ“ Kontakt i rozwÃ³j

Aplikacja jest aktywnie rozwijana. Planowane funkcje:
- ğŸ“Š Dashboard ze statystykami dÅ‚ugoterminowymi
- ğŸ“¤ Export raportÃ³w (PDF, Excel)
- ğŸ”” Powiadomienia real-time (WebSockets)
- ğŸŒ Multi-camera support
- ğŸ§  Bardziej zaawansowane modele AI (YOLO, Segmentation)

---

**Autor:** welding-detector team  
**Wersja:** 1.0  
**Data:** StyczeÅ„ 2026  

---

*Ten dokument zostaÅ‚ stworzony z myÅ›lÄ… o osobach nieznajÄ…cych siÄ™ na programowaniu. JeÅ›li coÅ› jest niejasne, zadaj pytanie!* ğŸ˜Š
